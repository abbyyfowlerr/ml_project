#imports
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    confusion_matrix,
    classification_report
)

import matplotlib.pyplot as plt
import seaborn as sns

# Load and Clean Data
df = pd.read_csv("C:/Users/sakla/Downloads/ML_Project/radiomics_features.csv")

# Add label: 0 = benign, 1 = malignant
df["label"] = df["ID"].apply(lambda x: 0 if "benign" in x.lower() else 1)

# Remove diagnostic columns-Radiomics generates many "diagnostics_" columns that contain metadata, not features
df = df.loc[:, ~df.columns.str.startswith("diagnostics_")]

# Prepare X and y
y = df["label"] # target variable x and y
X = df.drop(columns=["ID", "label"]) # all radiomics features.

# Select ONE important feature for regression exploration
# Picking first radiomic feature for demonstration 
feature_name = X.columns[0]  
X_single = X[[feature_name]]
y_single = y

print(f"\nUsing feature for regression studies: {feature_name}")

# LINEAR REGRESSION EXPERIMENT-plan to start with Linear and Polynomial Regression to study basic features relationship.
lin_reg = LinearRegression()
lin_reg.fit(X_single, y_single)

print("\n Linear Regression Analysis ")
print("Coefficient:", lin_reg.coef_[0])
print("Intercept :", lin_reg.intercept_)

# Plot linear relationship
plt.figure(figsize=(6,4))
plt.scatter(X_single, y_single, alpha=0.5, label="Data points")
plt.plot(X_single, lin_reg.predict(X_single), color="red", label="Linear Fit")
plt.xlabel(feature_name)
plt.ylabel("Label (0=Benign, 1=Malignant)")
plt.title("Linear Regression Relationship")
plt.legend()
plt.show()

# POLYNOMIAL REGRESSION (DEGREE 3)  #X, X², X³
poly = PolynomialFeatures(degree=3, include_bias=False) 
X_poly = poly.fit_transform(X_single)

poly_reg = LinearRegression()
poly_reg.fit(X_poly, y_single)

print("\n Polynomial Regression (Degree 3)")

# Polynomial curve plotting
X_sorted = np.sort(X_single.values.reshape(-1, 1), axis=0)
X_poly_sorted = poly.transform(X_sorted)
y_poly_pred = poly_reg.predict(X_poly_sorted)

plt.figure(figsize=(6,4))
plt.scatter(X_single, y_single, alpha=0.5, label="Data points")
plt.plot(X_sorted, y_poly_pred, color="green", label="Polynomial Fit (Degree 3)")
plt.xlabel(feature_name)
plt.ylabel("Label (0=Benign, 1=Malignant)")
plt.title("Polynomial Regression Relationship (Degree 3)")
plt.legend()
plt.show()


# Split Data - Train and Test # 80 % test and 20% train
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
) # stratify=y keeps benign/malignant ratio equal

#Evaluation
# This function trains a model and prints:Accuracy,Precision,Recall,Confusion Matrix,Classification Report and Confusion Matrix Heatmap all inside one reusable function.
def evaluate_classifier(name, model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    print(f"\n=== {name} ===")
    print(f"Accuracy : {acc:.3f}")
    print(f"Precision: {prec:.3f}")
    print(f"Recall   : {rec:.3f}")
    print("\nConfusion Matrix:\n", cm)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=["Benign", "Malignant"]))

    # Heatmap
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Benign", "Malignant"],
                yticklabels=["Benign", "Malignant"])
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Build Models
#Logistic 
log_reg_model = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000))
])

#Naves Bayes
nb_model = GaussianNB()

#SVM
svm_model = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", C=1.0, gamma="scale"))
])

# Model Evaluation
evaluate_classifier("Logistic Regression", log_reg_model)
evaluate_classifier("Naive Bayes", nb_model)
evaluate_classifier("SVM (RBF Kernel)", svm_model)
